library(readr)
library(naniar)
library(car)
library(forecast)
library(ggplot2)
library(MASS)
library("partykit")
library(naniar)
library(Metrics)
library(caret)
library(reshape)
Ab=read.csv("/Users/user2/Desktop/Rstudio workspace/Absenteeism_at_work.csv")
head(Ab)
cor(Ab)
Ab=Ab[,-1]
head(Ab)
summary(Ab)
str(Ab)
outliers <- boxplot(Ab$Absenteeism.time.in.hours, plot=FALSE)$out
Ab<- Ab[-which(Ab$Absenteeism.time.in.hours %in% outliers),]
#Ab$Height=(Ab$Height-mean(Ab$Height))/(max(Ab$Height)-min(Ab$Height))
#create dummy variables
Graduate <- ifelse(Ab$Education == '2', 1, 0)
Post <- ifelse(Ab$Education == '3', 1, 0)
MD <- ifelse(Ab$Education == '4', 1, 0)
#Hide Education column

Ab<- data.frame(Ab[,-12],
                Graduate = Graduate,
                Postgraduate = Post,
                MasterandDoctor = MD)
head(Ab)
boxplot(Ab$Absenteeism.time.in.hours)
plot(density(Ab$Absenteeism.time.in.hours))
#1,3% of the Absenteism are outliers
#Define Work average load in hours instead of minutes:
Ab$Work.load.Average.day=Ab$Work.load.Average.day/60
str(Ab)
gg_miss_var(Ab, show_pct = TRUE)
Ab$Month.of.absence=as.factor(Ab$Month.of.absence)
Ab=Ab[,-4]
#We split the dataset into training and test sets in an 60-40 ratio.
#With the reason of absence we predict the number of hours of absenteiism
set.seed(1)
train.index = sample(row.names(Ab), 0.6*dim(Ab)[1])
valid.index = setdiff(row.names(Ab), train.index)
train.df = Ab[train.index, ]
valid.df = Ab[valid.index, ]
# use lm() to run a linear regression of Price on all  predictors in the
# training set.
# use . after ~ to include all the remaining columns in train.df as predictors.
Ab.lm <- lm(Absenteeism.time.in.hours ~ ., data = train.df)
# use options() to ensure numbers are not displayed in scientific notation.
options(scipen = 999)
summary(Ab.lm)
#la recherche d'un R2 élevé ne doit pas être un but en soi..


library(car)
vif(Ab.lm)
#It seems that there is a real problem of multicollinearity between weight, height and body mass index, we can remove Body.mass.index and reverify the VIF coefficients:
Ab=Ab[,-17]
head(Ab)

#We redifine our dataset
set.seed(1)
train.index = sample(row.names(Ab), 0.8*dim(Ab)[1])
valid.index = setdiff(row.names(Ab), train.index)
train.df = Ab[train.index, ]
valid.df = Ab[valid.index, ]
Ab.lm <- lm(Absenteeism.time.in.hours ~ ., data = train.df)
summary(Ab.lm)
AIC(Ab.lm)
vif(Ab.lm)
#Problem of colinearity resolved
library(forecast)
# use predict() to make predictions on a new set.
Ab.lm.pred <- predict(Ab.lm, train.df)
V1=RMSE(Ab.lm.pred,train.df$Absenteeism.time.in.hours)
Ab.lm.pred <- predict(Ab.lm, valid.df)
V2=RMSE(Ab.lm.pred,valid.df$Absenteeism.time.in.hours)
summary(Ab.lm.pred)
options(scipen=999, digits = 0)
some.residuals <- valid.df$Absenteeism.time.in.hours[1:20] - Ab.lm.pred[1:20]
data.frame("Predicted" = Ab.lm.pred[1:20], "Actual" = valid.df$Absenteeism.time.in.hours[1:20],
           "Residual" = some.residuals)
options(scipen=999, digits = 3)
# use accuracy() to compute common accuracy measures.
accuracy(Ab.lm.pred, valid.df$Absenteeism.time.in.hours)
par(mfrow = c(2, 2))  # Split the plotting panel into a 2 x 2 grid
plot(Ab.lm)  # Plot the model information
par(mfrow = c(1, 1))
all.residuals <- valid.df$Absenteeism.time.in.hours - Ab.lm.pred
hist(all.residuals, breaks = 25, xlab = "Residuals", main = "",)
#We can identify a bell shape on the histogram, centered at 0, this shape indicates that more than 250 residuals(85.6% of the predicted values) are tt close to 0.
#produce residual vs. fitted plot
plot(valid.df$Absenteeism.time.in.hours,all.residuals)
boxplot(all.residuals)
#create Q-Q plot for residuals
qqPlot(all.residuals)


#Create density plot of residuals
plot(density(all.residuals))
#We can see that the density plot roughly follows a bell shape, although it is slightly skewed to the right.
#---------------------------------------------------------------------------------------------------------------
library(leaps)
#----------------------------Best Subset---------------
search <- regsubsets(Absenteeism.time.in.hours ~ ., data = train.df, nbest = 1, nvmax = dim(train.df)[2],
                     method = "exhaustive")
sum <- summary(search)
# show models
sum$which
# show metrics
sum$rsq
#rsq keeps increasing when adding new factors until we reach 19 predictive variables and then stabilizes.
sum$adjr2
sum$cp
#The Cp indicates that a model with  12  predictors is good. The dominant predictor in all models is Reason for absence,
#disciplinary failure and having sons playing important roles as well.
#----------------------backward regression------------

#AIC(Ab.lm.step)
Ab.lm.stepb <- step(Ab.lm, direction = "backward")#AIC=1177
summary(Ab.lm.stepb) 
#The backward selection dropped 14 predictors and kept 9.(as mentionned above for a cp optimization)
Ab.lm.stepb.pred <- predict(Ab.lm.stepb, train.df)
W1=RMSE(Ab.lm.stepb.pred,train.df$Absenteeism.time.in.hours)
Ab.lm.stepb.pred <- predict(Ab.lm.stepb, valid.df)
W2=RMSE(Ab.lm.stepb.pred,valid.df$Absenteeism.time.in.hours)
accuracy(Ab.lm.stepb.pred, valid.df$Absenteeism.time.in.hours)
plot(density(Ab.lm.stepb.pred))
plot(Ab.lm.stepb.pred-valid.df$Absenteeism.time.in.hours)
boxplot(Ab.lm.stepb.pred-valid.df$Absenteeism.time.in.hours)
#----------------------Forward regression------------

Ab.lm.stepf <- step(Ab.lm, direction = "forward")#AIC=1196
summary(Ab.lm.stepf) 
#The forward selection kept the 22 predictors
Ab.lm.stepf.pred <- predict(Ab.lm.stepf, train.df)
X1=RMSE(Ab.lm.stepf.pred,train.df$Absenteeism.time.in.hours)
Ab.lm.stepf.pred <- predict(Ab.lm.stepf, valid.df)
X2=RMSE(Ab.lm.stepf.pred,valid.df$Absenteeism.time.in.hours)

accuracy(Ab.lm.stepf.pred, valid.df$Absenteeism.time.in.hours)
plot(density(Ab.lm.stepf.pred))
plot(Ab.lm.stepf.pred-valid.df$Absenteeism.time.in.hours)
boxplot(Ab.lm.stepf.pred-valid.df$Absenteeism.time.in.hours)
#----------------------both regression------------
# set directions = to "both".
Ab.lm.stept <- step(Ab.lm, direction = "both")#AIC=1177
summary(Ab.lm.stept)
#The backward selection dropped 14 predictors and kept 8.(as mentionned above for a cp optimization)
Ab.lm.stept.pred <- predict(Ab.lm.stept, train.df)
Y1=RMSE(Ab.lm.stept.pred,train.df$Absenteeism.time.in.hours)
Ab.lm.stept.pred <- predict(Ab.lm.stept, valid.df)
Y2=RMSE(Ab.lm.stept.pred,valid.df$Absenteeism.time.in.hours)
accuracy(Ab.lm.stept.pred, valid.df$Absenteeism.time.in.hours)
plot(density(Ab.lm.stept.pred))
plot(Ab.lm.stept.pred-valid.df$Absenteeism.time.in.hours)
boxplot(Ab.lm.stept.pred-valid.df$Absenteeism.time.in.hours)

#models with
#smaller AIC and BIC values are considered better.
AIC(Ab.lm)
BIC(Ab.lm)
AIC(Ab.lm.stepb)
BIC(Ab.lm.stepb)
AIC(Ab.lm.stepf)
BIC(Ab.lm.stepf)
AIC(Ab.lm.stept)
BIC(Ab.lm.stept)
#The smaller AIC and BIC are recorded for the model of the backward selection, with 10 predictive variables:
#Reason.for.absence + Transportation.expense + Age + Disciplinary.failure + Son + Social.drinker + Social.smoker + Weight + Postgraduate
#Reason.for.absence,Month.of.absence,Day.of.the.week,Disciplinary.failure ,Son ,Social.drinker,Height,Spring,Summer
rmse.df <- data.frame(Regression = c("MLR","Backward","Forward","Backward/Forward"), training = c(V1,W1,X1,Y1), validation = c(V2, W2, X2,Y2))

rmse.df.melt <- melt(rmse.df)
rmse.plot <- ggplot(rmse.df.melt, aes(x=Regression, y=value, fill=variable)) + geom_bar(position="dodge", stat="identity") + ggtitle("Root mean square error (RMSE)") + theme_gray() 

rmse.plot

